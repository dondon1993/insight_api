{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wiki text extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```wikiExtractor``` function is to extact the introduction text of a certain wiki topic using API. Pure texts are returned.\n",
    "\n",
    "```wiki_find_redirect``` function is for redirecting the links if multiple topics are pointed to the same wiki page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def wiki_find_redirect(topic):\n",
    "    \n",
    "    url = 'https://en.wikipedia.org/w/api.php?action=query&titles=' + topic + '&&redirects&format=json'\n",
    "    r = requests.get(url)\n",
    "    pageid = list(json.loads(r.text)['query']['pages'].keys())[0]\n",
    "    title = json.loads(r.text)['query']['pages'][pageid]['title']\n",
    "    return title\n",
    "\n",
    "def wikiExtractor(topic):\n",
    "        \n",
    "    url = 'https://en.wikipedia.org/w/api.php?action=query&format=json&prop=extracts&exintro&explaintext&titles=' + topic\n",
    "    # if the full text of the article needs to be extracted\n",
    "    #url = 'https://en.wikipedia.org/w/api.php?action=query&format=json&prop=extracts&explaintext&titles=' + topic\n",
    "    page_r = requests.get(url)\n",
    "    page_content = page_r.content\n",
    "    page_json = json.loads(page_content)\n",
    "\n",
    "    pageid = list(page_json['query']['pages'].keys())[0]\n",
    "    text = page_json['query']['pages'][pageid]['extract']\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The event data is scraped from nyc.com and is stored as text. \n",
    "```preprocess``` is to clean the invalid charactors from the text, and prepare the data future steps. All event descriptions are stored separately using ```event_list_desc```. The TF-IDF model will be defined on the descriptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_path = '../insight_api_data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "with open(data_path + 'event_list.txt', 'r') as f:\n",
    "    event_list = json.load(f)\n",
    "\n",
    "event_list_desc = []\n",
    "for i, event in enumerate(event_list):\n",
    "    event_list_desc.append(event_list[i]['description'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Total number of events in the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "217"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(event_list_desc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i, event in enumerate(event_list_desc):\n",
    "    event_list_desc[i] = event_list_desc[i].replace('&#x27;', \"'\")\n",
    "    event_list_desc[i] = event_list_desc[i].replace('&#x2019;', \"'\")\n",
    "    event_list_desc[i] = event_list_desc[i].replace('B.C.', \"BC\")\n",
    "    event_list_desc[i] = event_list_desc[i].replace('A.D.', \"AD\")\n",
    "    event_list_desc[i] = event_list_desc[i].replace('&amp;', \"and\")\n",
    "\n",
    "    # remove ',' in numbers\n",
    "    event_list_desc[i] = re.sub('(\\d+),(\\d+)', lambda x: \"{}{}\".format(x.group(1).replace(',', ''), x.group(2)), event_list_desc[i])\n",
    "    event_list_desc[i] = re.sub('&#x(.*?);', ' ', event_list_desc[i])\n",
    "    event_list_desc[i] = re.sub('http(.+?) ', '', event_list_desc[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    \n",
    "    text = text.replace('\\n', ' ')\n",
    "    text = text.replace('&#x27;', \"'\")\n",
    "    text = text.replace('&#x2019;', \"'\")\n",
    "    \n",
    "    # remove the dots in B.C. and A.D.\n",
    "    text = text.replace('B.C.', \"BC\")\n",
    "    text = text.replace('A.D.', \"AD\")\n",
    "    \n",
    "    text = text.replace('&amp;', \"and\")\n",
    "\n",
    "    # remove ',' in numbers\n",
    "    text = re.sub('(\\d+),(\\d+)', lambda x: \"{}{}\".format(x.group(1).replace(',', ''), x.group(2)), text)\n",
    "    text = re.sub('&#x(.*?);', ' ', text)\n",
    "    text = re.sub('http(.+?) ', '', text)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keyword extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are three types of keywords been considered in the model:\n",
    "- Named entities, e.g. \"Claude Monet\", \"Africa\".\n",
    "- Proper noun(s) + normal noun, e.g. \"Greek sculptures\".\n",
    "- Adjectives + normal noun, e.g. \"contempory arts\".\n",
    "\n",
    "Function ```key_phrases```, ```nnp_nn``` and ```jj_nn``` are defined to find these three types of keywords."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Named entity recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spacy is used for *named entitiy recognition (NER)*. The types of entities been considered such as person, location, work of art, are set by ```spacy_labels```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "spacy_labels = set(['PERSON', 'NORP', 'ORG', 'GPE', 'LOC', 'WORK_OF_ART'])\n",
    "def NER_phrases(text):\n",
    "    phrases = {}\n",
    "    spacy_phrases = []\n",
    "    doc = nlp(text)\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ in spacy_labels:\n",
    "            phrase = ent.text\n",
    "            phrase = phrase.replace('the ', '')\n",
    "            phrase = phrase.replace('The ', '')\n",
    "            phrase = phrase.replace(\"'s\", \"\")\n",
    "            spacy_phrases.append(phrase)\n",
    "    phrases['spacy_phrases'] = spacy_phrases\n",
    "    phrases['nnp_nn'] = nnp_nn(text)\n",
    "    phrases['jj_nn'] = jj_nn(text)\n",
    "    \n",
    "    return phrases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PoS tagging and pattern recognition\n",
    "In order to find patterns like proper noun(s) + normal noun and adjectives + normal nouns, part of speech tagging (PoS) is needed. NLTK packaged is used for tagging and pattern recognition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```dog2tag``` function converts texts into a list of tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def doc2tag(text):\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    tag_list = []\n",
    "    for s in sentences:\n",
    "        tokens = nltk.word_tokenize(s)\n",
    "        text_tagged = nltk.pos_tag(tokens)\n",
    "        pair = [(word, pos) for (word, pos) in text_tagged]\n",
    "        tag_list.extend(pair)\n",
    "    \n",
    "    return tag_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "patterns like proper noun(s) + normal noun and adjective + normal noun are captured by regex-like patterns defined in NLTK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def nnp_nn(text):\n",
    "    patterns = \"NNP_NN: {<NNP>+(<NNS>|<NN>+)}\" # at least one NNP followed by NNS or at least one NN\n",
    "    parser = nltk.RegexpParser(patterns)\n",
    "    p = parser.parse(doc2tag(text))\n",
    "    phrase = []\n",
    "    for node in p:\n",
    "        if type(node) is nltk.Tree:\n",
    "            phrase_str = ''\n",
    "            for w in node:\n",
    "                phrase_str += w[0]\n",
    "                phrase_str += ' '\n",
    "            phrase_str = phrase_str.strip()\n",
    "            phrase.append(phrase_str)\n",
    "    return phrase\n",
    "\n",
    "def jj_nn(text):\n",
    "    patterns = \"NNP_NN: {<JJ>+(<NN>+)}\"\n",
    "    parser = nltk.RegexpParser(patterns)\n",
    "    p = parser.parse(doc2tag(text))\n",
    "    phrase = []\n",
    "    for node in p:\n",
    "        if type(node) is nltk.Tree:\n",
    "            phrase_str = ''\n",
    "            for w in node:\n",
    "                phrase_str += w[0]\n",
    "                phrase_str += ' '\n",
    "            phrase_str = phrase_str.strip()\n",
    "            phrase.append(phrase_str)\n",
    "    return phrase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The keywords/phrases are captured and converted into tokens, after lemmatization and convertion into lower case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_key_tokens(text):\n",
    "    \n",
    "    phrases = {}\n",
    "    spacy_phrases = []\n",
    "    doc = nlp(text)\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ in spacy_labels:\n",
    "            phrase = ent.text\n",
    "            phrase = phrase.replace('the ', '')\n",
    "            phrase = phrase.replace('The ', '')\n",
    "            phrase = phrase.replace(\"'s\", \"\")\n",
    "            spacy_phrases.append(phrase)\n",
    "    phrases['spacy_phrases'] = spacy_phrases\n",
    "    phrases['nnp_nn'] = nnp_nn(text)\n",
    "    phrases['jj_nn'] = jj_nn(text)\n",
    "    \n",
    "    # add all phrases to the tokens; split and lemmatize nnp_nn, jj_nn\n",
    "    tokens = set()\n",
    "    for p in phrases['spacy_phrases']:\n",
    "        tokens.add(p.lower())\n",
    "    for p in phrases['nnp_nn']:\n",
    "#         tokens.add(p.lower())\n",
    "#         tokens.update(p.lower().split(' '))\n",
    "        tokens.add(lemmatizer.lemmatize(p.lower()))\n",
    "        for word in p.lower().split(' '):\n",
    "            tokens.add(lemmatizer.lemmatize(word))\n",
    "    for p in phrases['jj_nn']:\n",
    "#         tokens.add(p.lower())\n",
    "#         tokens.update(p.lower().split(' '))\n",
    "        tokens.add(lemmatizer.lemmatize(p.lower()))\n",
    "        for word in p.lower().split(' '):\n",
    "            tokens.add(lemmatizer.lemmatize(word))\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The event matching uses TF-IDF algorithm. TF stands for term frequency, the frequency of a word appearing in a document. IDF stands for inverse document frequency, the logarithm of the inverse of frequency that a word appears in all the documents. The dimension of TF-IDF vector is determined by the number of terms been considered. In this model, IDF is calculated only from the event descriptions; TF is calculated for each document (either a wiki article or an event description)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build a list of tokens of keywords in all the event descriptions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "event_list_key_tokens = []\n",
    "for t in event_list_desc:\n",
    "    event_list_key_tokens.append(extract_key_tokens(t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Put all the keyword tokens into a bag of words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BoW = set()\n",
    "for event in event_list_key_tokens:\n",
    "    BoW.update(event)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inverse document frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The inverse document frequency (IDF) is defined as log(doc_count/word_count_in_doc). Stop words are not included for the IDF calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Import the common English stopwords, and add customary stopwords.\n",
    "stop = set(stopwords.words('english'))\n",
    "stop_words = set(['event', 'collection', 'street', 'many', \n",
    "                  'exhibitions', 'works', 'monday', 'tuesday', \n",
    "                  'wednesday', 'thursday', 'friday', 'saturday', \n",
    "                  'sunday', 'new', 'york', 'new york', 'new york city',\n",
    "                  'visit', 'museum', 'world', 'department', 'NYC'\n",
    "                 ])\n",
    "stop.update(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "idf_dict = defaultdict(int)\n",
    "D = len(event_list_desc)\n",
    "for t in BoW:\n",
    "    if t in stop:\n",
    "        continue\n",
    "    for event in event_list_key_tokens:\n",
    "        if t in event:\n",
    "            idf_dict[t] += 1\n",
    "    idf_dict[t] = np.log(D/idf_dict[t])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this model, only uni-, bi- and tri-grams are considered for the IDF space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ngram = 3\n",
    "key_tokens = set([key for key in idf_dict.keys() if len(key.split()) <= ngram])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The total number of tokens been included is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4242"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(key_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Term frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ```tf_idf``` function calculate the TF-IDF vector based on the idf values of the token considered. \n",
    "TF-IDF vectors are calculated as **(word_count/total_word_count) * log(doc_count/word_count_in_doc)** for each dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tf_idf(text, key_tokens, idf_dict, normalize=True, ngram=3):\n",
    "    \n",
    "    tf_idf_dict = defaultdict(int)\n",
    "    \n",
    "    text = preprocess(text)\n",
    "    text = text.lower()\n",
    "    \n",
    "    # tokens to be used for tf-idf\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    \n",
    "    token_list = []\n",
    "    for i in range(1, ngram+1):\n",
    "        token_list.extend(nltk.ngrams(tokens, i))\n",
    "    token_list = [' '.join(token) for token in token_list]\n",
    "    \n",
    "    # lemmatize the tokens\n",
    "    for i, token in enumerate(token_list):\n",
    "        token_list[i] = lemmatizer.lemmatize(token)\n",
    "    \n",
    "    # calculate the normalized term frequency using count of total uni, bi, trigrams\n",
    "    terms = len(token_list)\n",
    "    \n",
    "    # initialize the tf_idf_dict dictionary\n",
    "    for token in key_tokens:\n",
    "        tf_idf_dict[token] = 0\n",
    "    \n",
    "    # count only the tokens in the key_tokens (uni-, bi- and tri-grams)\n",
    "    for token in token_list:\n",
    "        if token in key_tokens:\n",
    "            tf_idf_dict[token] += 1\n",
    "    \n",
    "    # if normalized, the the tf-idf vector will be devided by the number of terms in the text\n",
    "    for key in tf_idf_dict.keys():\n",
    "        \n",
    "        if normalize:\n",
    "            tf_idf_dict[key] = tf_idf_dict[key] / terms * idf_dict[key]\n",
    "        else:\n",
    "            tf_idf_dict[key] = tf_idf_dict[key] * idf_dict[key]\n",
    "            \n",
    "        tf_idf_dict[key] = tf_idf_dict[key] * idf_dict[key]\n",
    "    \n",
    "    # convert the dictionary into an np array\n",
    "    tf_idf_vec = np.zeros((len(key_tokens),))\n",
    "    for i, key in enumerate(key_tokens):\n",
    "        tf_idf_vec[i] = tf_idf_dict[key]\n",
    "        \n",
    "    # returns a 1d np array as tf-idf vector\n",
    "    return tf_idf_vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each event has a feature vector. The inner product between the feature vector of an event and a wiki article is the cosine similarity between them. Once the similarity is over a threshold, the event is considered to be *relevant* to the article. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An event matrix needs to be constructed to include all the feature vectors for the events. Some events are not suitable for recommendation (not cultural, e.g. magic show) and are excluded from the recommendation candidate list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "event_set = set()\n",
    "with open(data_path + 'event_list_wiki_dict_picked.txt', 'r') as f:\n",
    "    event_list_wiki_dict_picked = json.load(f)\n",
    "for key in event_list_wiki_dict_picked.keys():\n",
    "    if len(event_list_wiki_dict_picked[key]) != 0:\n",
    "        event_set.add(key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lookup tables from event matrix index to event list are constructed, to retrive the event description text once the recommendation is made."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "event_matrix2list_dict = {}\n",
    "list2event_matrix_dict = {}\n",
    "matrix_ind = 0\n",
    "for key in event_list_wiki_dict_picked.keys():\n",
    "    if len(event_list_wiki_dict_picked[key]) != 0:\n",
    "        event_matrix2list_dict[str(matrix_ind)] = key\n",
    "        list2event_matrix_dict[key] = str(matrix_ind)\n",
    "        matrix_ind += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Event matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construct the event matrix. All the event feature vectors are normalized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "vector_dim = len(key_tokens)\n",
    "event_count = len(event_matrix2list_dict)\n",
    "event_matrix = np.zeros((event_count, vector_dim))\n",
    "\n",
    "for key in event_matrix2list_dict.keys():\n",
    "    event_matrix_ind = int(key)\n",
    "    event_list_ind = int(event_matrix2list_dict[key])\n",
    "    event_desc = event_list_desc[event_list_ind]\n",
    "    event_matrix[event_matrix_ind, :] = tf_idf(event_desc, key_tokens, idf_dict)\n",
    "    \n",
    "event_matrix = normalize(event_matrix, axis=1, norm='l2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The threshold is determined by maximizing the F1 score on the test data (the steps are not included in this notebook)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "threshold = 0.05\n",
    "\n",
    "def predict(event_matrix, vec_norm, threshold):\n",
    "    \n",
    "    prod = vec_norm.dot(event_matrix.T)\n",
    "    # returned values are the indices of the selected events, or the event matrix indeces\n",
    "    return list(np.nonzero(prod[0, :] > threshold)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, assume the user is viewing wiki topic \"Ancient Egypt\". The intro text of the wiki page is extracted and vectorized using the tf-idf function previously defined. Events with cosine similarity higher than the threshold is returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "topic = 'Ancient_Egypt'\n",
    "wiki_text = wikiExtractor(topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Ancient Egypt was a civilization of ancient North Africa, concentrated along the lower reaches of the Nile River in the place that is now the country Egypt. Ancient Egyptian civilization followed prehistoric Egypt and coalesced around 3100 BC (according to conventional Egyptian chronology) with the political unification of Upper and Lower Egypt under Menes (often identified with Narmer). The history of ancient Egypt occurred as a series of stable kingdoms, separated by periods of relative instability known as Intermediate Periods: the Old Kingdom of the Early Bronze Age, the Middle Kingdom of the Middle Bronze Age and the New Kingdom of the Late Bronze Age.\\nEgypt reached the pinnacle of its power in the New Kingdom, ruling much of Nubia and a sizable portion of the Near East, after which it entered a period of slow decline. During the course of its history Egypt was invaded or conquered by a number of foreign powers, including the Hyksos, the Libyans, the Nubians, the Assyrians, the Achaemenid Persians, and the Macedonians under the command of Alexander the Great. The Greek Ptolemaic Kingdom, formed in the aftermath of Alexander's death, ruled Egypt until 30 BC, when, under Cleopatra, it fell to the Roman Empire and became a Roman province.The success of ancient Egyptian civilization came partly from its ability to adapt to the conditions of the Nile River valley for agriculture. The predictable flooding and controlled irrigation of the fertile valley produced surplus crops, which supported a more dense population, and social development and culture. With resources to spare, the administration sponsored mineral exploitation of the valley and surrounding desert regions, the early development of an independent writing system, the organization of collective construction and agricultural projects, trade with surrounding regions, and a military intended to assert Egyptian dominance. Motivating and organizing these activities was a bureaucracy of elite scribes, religious leaders, and administrators under the control of a pharaoh, who ensured the cooperation and unity of the Egyptian people in the context of an elaborate system of religious beliefs.The many achievements of the ancient Egyptians include the quarrying, surveying and construction techniques that supported the building of monumental pyramids, temples, and obelisks; a system of mathematics, a practical and effective system of medicine, irrigation systems and agricultural production techniques, the first known planked boats, Egyptian faience and glass technology, new forms of literature, and the earliest known peace treaty, made with the Hittites. Ancient Egypt has left a lasting legacy. Its art and architecture were widely copied, and its antiquities carried off to far corners of the world. Its monumental ruins have inspired the imaginations of travelers and writers for centuries. A new-found respect for antiquities and excavations in the early modern period by Europeans and Egyptians led to the scientific investigation of Egyptian civilization and a greater appreciation of its cultural legacy.\""
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wiki_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wiki_vec = tf_idf(wiki_text, key_tokens, idf_dict, normalize=True, ngram=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Event:  Ancient Egyptian Art\n",
      "The collection of ancient Egyptian art at the Metropolitan Museum ranks among the finest outside Cairo. It consists of approximately 36,000 objects of artistic, historical, and cultural importance, dating from the Paleolithic to the Roman period (ca. 300,000 B.C.&#x2013;4th century A.D.). More than half of the collection is derived from the Museum&#x27;s thirty-five years of archaeological work in Egypt, initiated in 1906 in response to increasing public interest in the culture of ancient Egypt. Today, virtually the entire collection is on display in thirty-two major galleries and eight study galleries, with objects arranged chronologically. Overall, the holdings reflect the aesthetic values, history, religious beliefs, and daily life of the ancient Egyptians over the entire course of their great civilization.  The Department of Egyptian Art is particularly well known for the Old Kingdom mastaba (offering chapel) of Perneb (ca. 2450 B.C.); a set of Middle Kingdom wooden models from the tomb of Meketre at Thebes (ca. 1990 B.C.); jewelry of Princess Sit-hathor-yunet of Dynasty 12 (ca. 1897&#x2013;1797 B.C.); royal portrait sculpture of Dynasty 12 (ca. 1991&#x2013;1783 B.C.); and statuary of the female pharaoh Hatshepsut of Dynasty 18 (ca. 1473&#x2013;1458 B.C.). The department also exhibits its invaluable collection of watercolor facsimiles, most of which are copies of Theban tomb paintings produced between 1907 and 1937 by members of the Graphic Section of the Museum&#x27;s Egyptian Expedition.\n",
      "\n",
      "\n",
      "Event:  Egypt Reborn: Art for Eternity\n",
      "In April, 2003, the Brooklyn Museum completed the reinstallation of its world-famous Egyptian collection, a process that took ten years. Three new galleries joined the four existing ones that had been completed in 1993 to tell the story of Egyptian art from its earliest known origins (circa 3500 B.C.) until the period when the Romans incorporated Egypt into their empire (30 B.C.&#x2013;A.D. 395). Additional exhibits illustrate important themes about Egyptian culture, including women&#x27;s roles, permanence and change in Egyptian art, temples and tombs, technology and materials, art and communication, and Egypt and its relationship to the rest of Africa. More than 1,200 objects&#x2014; comprising sculpture, relief, paintings, pottery, and papyri&#x2014;are now on view, including such treasures as an exquisite chlorite head of a Middle Kingdom princess, an early stone deity from 2650 B.C., a relief from the tomb of a man named Akhty-hotep, and a highly abstract female terracotta statuette created over five thousand years ago.  The title of the installation refers to a central theme of Egyptian life and to the rebirth of Egyptian art at the Brooklyn Museum. The ancient Egyptians created many of the objects now on view to assist in the process of rebirth from this world to the next. This unifying idea led to an artistic conservatism in Egyptian culture that disguises stylistic changes. The balance between permanence and change is a theme that resonates throughout the installation&#x27;s seven galleries.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "events = predict(event_matrix, wiki_vec.reshape(1, -1), threshold)\n",
    "for ix in events:\n",
    "    print('Event: ', event_list[int(event_matrix2list_dict[str(ix)])]['name'])\n",
    "    print(event_list[int(event_matrix2list_dict[str(ix)])]['description'])  \n",
    "    print('\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the wiki topic \"Ancient Egypt\", two events are recommended: \"Ancient Egyptian Art\" and \"Event:  Egypt Reborn: Art for Eternity\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
